# Ego-Interaction: A Multiple Hand-to-Rigid and Non-Rigid Object Interaction Dataset
Code and Data for the Ego-Interaction Dataset

![teaser](https://user-images.githubusercontent.com/25514442/104743868-17958680-5744-11eb-96d2-26f17782ba12.png)

## Table of Contents
  * [Abstract](#abstract)
  * [Dataset](#dataset)
    * [Contents](#contents)
    * [Layout](#layout)
	* [Access](#access)
	* [Revisions](#access)
  * [Code](#code)
    * [Dependencies](#dependencies)
  * [Citation](#citation)
  * [Contact](#contact)
  
  ## Abstract
  
Novel and immersive VR and AR experiences can be created by transporting physical objects into virtual worlds. This requires a fast and robust algorithm for tracking the behaviour of rigid and nonrigid objects within Egocentric hand-object interaction sequences. In turn, the design of such an algorithm requires high quality and varied data. Existing hand-object datasets are largely limited to rigid object interactions, 3rd person views or are aimed towards hand tracking or action recognition applications and so do not provide the ground truth object pose or shape. Moreover, those datasets which do show non-rigid object interactions provide no ground truth 3D data for the object shape and pose and instead only provide 2D annotations.

We address these limitations in this paper, by presenting a new dataset – Ego-Interaction – the first egocentric hand-object interaction dataset with 3d ground truth data for both rigid and non-rigid objects. The Ego-Interaction dataset contains 92 sequences with 4 rigid, 1 articulated and 4 non-rigid objects and demonstrates handobject interactions with 1 and 2 hands. We also outline an approach for creating additional synthetic sequences, by augmenting the data provided in our dataset and discuss the potential future direction of hand-object tracking datasets. Our dataset and scripts to generate synthetic data will be made publicly available for research purposes.
  
  ## Dataset
  
  ### Contents
  
  ## Code
  
  ## Contact
  
  This code is maintained by Catherine Taylor (c.taylor3@bath.ac.uk).
